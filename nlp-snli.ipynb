{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librarties \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import nltk \n",
    "import spacy\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(134)\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab from pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab from fasttext embeddings \n",
    "\n",
    "def build_vocab(word2vec_source, max_vocab_size): \n",
    "    \"\"\" Takes pretrained word2vec source path, limits to max_vocab_size, and returns:  \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices \n",
    "        - word_emb: dictionary representing word embeddings \n",
    "    \"\"\"\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_source, limit=max_vocab_size)\n",
    "    id2token = word2vec_model.index2word\n",
    "    token2id = dict(zip(id2token, range(2, 2+len(id2token))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    word_emb = {token2id[w]: word2vec_model[w] for w in word2vec_model.vocab}\n",
    "\n",
    "    return id2token, token2id, word_emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token, token2id, word_emb = build_vocab('fasttext_word2vec/wiki-news-300d-1M.vec', MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert word_emb from dict to matrix \n",
    "pretrained_weights = np.zeros((MAX_VOCAB_SIZE+2, 300))\n",
    "for idx in word_emb: \n",
    "    try: \n",
    "        pretrained_weights[idx] = word_emb[idx]\n",
    "    except KeyError: \n",
    "        pretrained_weights[idx] = np.random.normal(size=(300,))\n",
    "pretrained_weights = torch.from_numpy(pretrained_weights.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and convert to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 1000 20000 5000\n"
     ]
    }
   ],
   "source": [
    "# load data into pandas dataframe \n",
    "snli_train = pd.read_table('hw2_data/snli_train.tsv')\n",
    "snli_val = pd.read_table('hw2_data/snli_val.tsv')\n",
    "mnli_train = pd.read_table('hw2_data/mnli_train.tsv')\n",
    "mnli_val = pd.read_table('hw2_data/mnli_val.tsv')\n",
    "print(len(snli_train), len(snli_val), len(mnli_train), len(mnli_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young girl in a pink shirt sitting on a dock...</td>\n",
       "      <td>A young girl watching the sunset over the water .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman is smiling while the man next to her i...</td>\n",
       "      <td>Two people are next to each other .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Across the river , you can see a large building .</td>\n",
       "      <td>The large building is full of apartments and t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a man in white shorts and a black shirt is par...</td>\n",
       "      <td>A man is riding a jetski on the ocean .</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four black dogs run together on bright green g...</td>\n",
       "      <td>Four dogs are preparing to be launched into sp...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A young girl in a pink shirt sitting on a dock...   \n",
       "1  A woman is smiling while the man next to her i...   \n",
       "2  Across the river , you can see a large building .   \n",
       "3  a man in white shorts and a black shirt is par...   \n",
       "4  Four black dogs run together on bright green g...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0  A young girl watching the sunset over the water .        neutral  \n",
       "1                Two people are next to each other .     entailment  \n",
       "2  The large building is full of apartments and t...        neutral  \n",
       "3            A man is riding a jetski on the ocean .  contradiction  \n",
       "4  Four dogs are preparing to be launched into sp...  contradiction  "
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to convert pandas df to lists of word indices and labels \n",
    "\n",
    "label_dict = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "\n",
    "def token_to_index_datum(datum_tokens, token2id): \n",
    "    \"\"\" Converts a list of tokens and converts it to a list of token indices for one datum \"\"\" \n",
    "    index_list = [token2id[token] if token in token2id else UNK_IDX for token in datum_tokens]\n",
    "    return index_list \n",
    "\n",
    "def df_to_list(data_df, token2id): \n",
    "    \"\"\" Takes train/val data as pandas df and returns: \n",
    "        - list of lists of word indices representing first sentence \n",
    "        - list of lists of word indices representing second sentence \n",
    "        - list of ground truth labels indicating entailment/contradiction/neutrality of two sentences \n",
    "    \"\"\"\n",
    "    sent1 = [token_to_index_datum(sent.split(), token2id) for sent in data_df['sentence1'].tolist()] \n",
    "    sent2 = [token_to_index_datum(sent.split(), token2id) for sent in data_df['sentence2'].tolist()] \n",
    "    labels = [label_dict[label] for label in data_df['label'].tolist()] \n",
    "    return sent1, sent2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to indices \n",
    "snli_train_sent1, snli_train_sent2, snli_train_labels = df_to_list(snli_train, token2id)\n",
    "snli_val_sent1, snli_val_sent2, snli_val_labels = df_to_list(snli_val, token2id)\n",
    "mnli_train_sent1, mnli_train_sent2, mnli_train_labels = df_to_list(mnli_train, token2id)\n",
    "mnli_val_sent1, mnli_val_sent2, mnli_val_labels = df_to_list(mnli_val, token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build PyTorch Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create dataloader \n",
    "\n",
    "class SNLI_Dataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_list, sent2_list, label_list): \n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in the sent and label lists, along with max_sent_length \n",
    "        \"\"\"\n",
    "        self.sent1_list = sent1_list\n",
    "        self.sent2_list = sent2_list \n",
    "        self.label_list = label_list\n",
    "        assert (len(self.sent1_list) == len(self.sent2_list) == len(self.label_list))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.label_list)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called, outputs a list of tokens, lengths of lists, and label of the data point\n",
    "        \"\"\"\n",
    "        sent1_idx = self.sent1_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_idx = self.sent2_list[key][:MAX_SENTENCE_LENGTH]    \n",
    "        sent1_len = len(sent1_idx)\n",
    "        sent2_len = len(sent2_idx)\n",
    "        label = self.label_list[key]\n",
    "        return [sent1_idx, sent2_idx, sent1_len, sent2_len, label]\n",
    "    \n",
    "def collate_func(batch): \n",
    "    \"\"\"Customized function for DataLoader that dynamically pads the batch \n",
    "       so that all data have the same length\"\"\" \n",
    "    \n",
    "    sents1 = [] \n",
    "    sents2 = [] \n",
    "    sent1_lens = [] \n",
    "    sent2_lens = [] \n",
    "    labels = [] \n",
    "\n",
    "    for datum in batch:\n",
    "        sent1_lens.append(datum[2])\n",
    "        sent2_lens.append(datum[3])\n",
    "        labels.append(datum[4])\n",
    "        \n",
    "        # pad data before appending \n",
    "        padded_vec1 = np.pad(array=np.array(datum[0]), \n",
    "                             pad_width = ((0, MAX_SENTENCE_LENGTH - datum[2])), \n",
    "                             mode = 'constant', constant_values = 0)\n",
    "        padded_vec2 = np.pad(array=np.array(datum[1]), \n",
    "                             pad_width = ((0, MAX_SENTENCE_LENGTH - datum[3])), \n",
    "                             mode = 'constant', constant_values = 0)\n",
    "        sents1.append(padded_vec1)\n",
    "        sents2.append(padded_vec2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(sents1)), torch.from_numpy(np.array(sents2)), \n",
    "            torch.LongTensor(sent1_lens), torch.LongTensor(sent2_lens), torch.LongTensor(labels)]\n",
    "\n",
    "def create_data_loader(sent1_list, sent2_list, label_list): \n",
    "    \"\"\" Takes index lists of sentence1, setence2, and labels for a given dataset and returns a data loader \"\"\"\n",
    "    data_set = SNLI_Dataset(sent1_list, sent2_list, label_list)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=data_set, batch_size=BATCH_SIZE, \n",
    "                                              collate_fn=collate_func, shuffle=False)\n",
    "    return data_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders \n",
    "snli_train_loader = create_data_loader(snli_train_sent1, snli_train_sent2, snli_train_labels)\n",
    "snli_val_loader = create_data_loader(snli_val_sent1, snli_val_sent2, snli_val_labels)\n",
    "mnli_train_loader = create_data_loader(mnli_train_sent1, mnli_train_sent2, mnli_train_labels)\n",
    "mnli_val_loader = create_data_loader(mnli_val_sent1, mnli_val_sent2, mnli_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[  106,   802,  1830,     8,     9,  6265,  7167,  4388,    17,     9,\n",
      "         12229,  5335,     9,   563,     6,   358,     4,     0,     0,     0],\n",
      "        [  106,   994,    15,  9766,   131,     3,   347,   424,     7,    86,\n",
      "            15,  3356,    17,     9,  2031,  1615,    19,     9,  2607,    17],\n",
      "        [17790,     3,  1938,     2,    32,    84,   138,     9,   542,   402,\n",
      "             4,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    9,   347,     8,   909, 11907,     5,     9,   884,  7167,    15,\n",
      "             1,    17,     3,  5656,     0,     0,     0,     0,     0,     0],\n",
      "        [ 3543,   884,  3135,   441,   549,    17,  4808,  1826,  5472,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  1572, 10015,    17,    86,  7455,     8,     3,   358,   529,\n",
      "            19, 35923,     4,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 3039,  3199,    29,     9,   389,  1036,    19,   884,  7602,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   363, 10308,     9, 12190,  1213,     4,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 6525,  5211,     9,  1372, 10890,   226,     9,   428,  1784,     2,\n",
      "            19,     9,   634,    67,     8,     3,  1411,     4,     0,     0],\n",
      "        [ 1442,   802,   271,  8867,     2,    46,     1,     5,    46, 23276,\n",
      "             2,  3302,   756,  1372,     5,   884, 10252,     5, 11468,     9],\n",
      "        [   22,   884,  2371,    15,  8795,    94,     3,   909,  8073,     8,\n",
      "             3,  5472,     4,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1442,   802,  2293,    34,  1663,    91,     3,   740,  2420,    16,\n",
      "            66,   498,     4,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   802,  2034,     8,     9, 44445,  1472,     1,  9701,     8,\n",
      "            44,     1,     5,  5362,    44,  3653,    81,  3173,  2301,   608],\n",
      "        [  106,   994,    19,   389,  2956,    15,  1935,     7,   107,   355,\n",
      "             8,  4301,     4,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   194,     6,    67,  3173,    29,    50,   529, 12245,    19,\n",
      "             9,  6885,  1380,     8,     3, 25491,     4,     0,     0,     0],\n",
      "        [    9,   347,  3535,     3,  3176,   131,   811,     3,  3147,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  417,    15,     9,  1830, 26311,    16,    50,   534,     5,     9,\n",
      "           347,    15,  2344,   424,     7,    86,  9766,     2,     5,    50],\n",
      "        [  106, 37656,     1,   226,    19,    44,  6885,    17,     3, 14871,\n",
      "           529,    19,     9,   909, 11147,    17,     4,     0,     0,     0],\n",
      "        [  508,   363, 34071,   131,   244,   350,    17,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   347,     1,     9,  2328,   226,    44,  9909,    29,     9,\n",
      "         10744, 23124,     4,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  1372,   714,   812,  1785,  4031,   701,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1442,   355,    34,  2344,   701,    50, 22342,   194,     6,   289,\n",
      "         15340,  7602,     4,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  2371,  8304,     9, 11362,  8492,  5704,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   363,     8,     9,   909,  5639,     5,  3466,   494,  7161,\n",
      "             9,  2645,   131,     9,   347,     8,    50,  6331,  7167,  1445],\n",
      "        [ 1202,   350,    29,  9468,    91,  1642,    29,   685,     4,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1202,  4388,   226,     1,  4503, 14100,   538,   590,     9,  1938,\n",
      "            17,     9, 25114,   195,     4,     0,     0,     0,     0,     0],\n",
      "        [  106,   347,  2784,    17,  4518,    17,   494,     6,     9,  1837,\n",
      "          9878,     4,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1442,   355,    34,  5211,     9, 12190,     4,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  1830,  8694,     8,     9,  6265,     1,     5,  6265,     5,\n",
      "           909, 24337, 14646,  1746,     8,     3,  4854,    29,     3,  3839],\n",
      "        [ 5535,     8,     9,     1,    19,    50,  6331,  7167,    17,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   471,   817,  3173,   608,     8,     9,  2729,     4,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 5535,    17,     9,   471, 34541,   504,     9,   471,  9619,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  106,   802,  1830,  2255,     3, 13985,    94,     3,   358,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1442,    67,    34,   424,     7,   204,    71,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,   542,   402,    15,   490,     6,  8417,     5,  8809,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   347,    15,  5211,     9,     1,    17,     3,  5656,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 3543,  3135,    34,  4721,     7,    40,  2193,    90,   595,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  417,    15,     9,   271, 12210,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 7734,    29,     9,   836,  1036,    19,  1372,  7602,     4,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,   347,    15,  4382,    44, 12190,     8,     9,   714,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  363,  5211,     9,  6885,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1442,   271,    34,  3302, 10252,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,  2371,    15,  8795,   387,     4,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,   107,    67,    34,  1663,    91,     9,  1036,     4,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   802,  2034,     8,     9, 44445,  1472,     1,   229,     1,\n",
      "          5367,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  417,    24,     9,     1,  5743,  1935,    19,   107,    67,     4,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  417,    34,    67,   529,     6,     3, 12245,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106, 47353,     1,  1746,  3147,    17, 26436,  1472,    16,  5617,\n",
      "             4,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  1830,    15, 26311,    16,     9,  2485,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22, 37656,    15,  6810,     6,  5211,    44,  6885,     4,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   994,    15, 34071,    19,   103,    46,   205,     4,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   347,    15,   939,     9, 10744, 23124,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   812,  9701,     1,     8,     9,  8274,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    3,   355,    34,   410,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,  2371,    26,  1891,    15,    91,  2212,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,   347,  3302,     9,  5639,  4518,     9,   428,  2645,   131,\n",
      "           167,   347,  1445,   226,    29,    44,  5123,     4,     0,     0],\n",
      "        [   22,    67,    34,  4813,     4,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1202,    34,   529,     4,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,   347,    15,  2454,     9,   487,     4,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  2040,    67,  3251,     9, 12190,     4,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  106,  1830,  1746,    29,     3,  3839,     4,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  417,    15,     9,   560,    17,     3,  7167,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    3,   817,    15,    96,     7,  3199,    19,    21,    26,   251,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   22,   817,    15,  2344,     8,     3,  9619,     4,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([17, 20, 11, 14, 10, 13, 10,  7, 18, 20, 13, 13, 20, 13, 17, 10, 20, 17,\n",
      "         8, 13,  8, 13,  8, 20,  9, 15, 12,  7, 20, 10,  9, 10])\n",
      "tensor([10,  8,  9, 10, 10,  5,  9, 10,  4,  5,  6,  9, 11, 10,  8, 11,  8,  9,\n",
      "         9,  8,  8,  4,  8, 18,  5,  4,  7,  7,  7,  8, 10,  8])\n",
      "tensor([1, 2, 1, 0, 0, 2, 0, 1, 2, 2, 1, 0, 0, 2, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1,\n",
      "        1, 2, 1, 2, 2, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (sent1_idx, sent2_idx, sent1_len, sent2_len, label) in enumerate(snli_train_loader):\n",
    "    print(i)\n",
    "    print(sent1_idx)\n",
    "    print(sent2_idx)\n",
    "    print(sent1_len)\n",
    "    print(sent2_len)\n",
    "    print(label)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_model(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, fc_out_size, num_classes, pretrained_weights):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super().__init__() \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding1 = nn.Embedding.from_pretrained(pretrained_weights, freeze=True)\n",
    "        self.embedding2 = nn.Embedding.from_pretrained(pretrained_weights, freeze=True)\n",
    "        self.rnn1 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.rnn2 = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, fc_out_size)\n",
    "        self.fc2 = nn.Linear(fc_out_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x1, x2, x1_lengths, x2_lengths):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size = x1.size()[0]\n",
    "\n",
    "        self.hidden1 = self.init_hidden(batch_size)\n",
    "        self.hidden2 = self.init_hidden(batch_size)\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding1(x1)\n",
    "        embed2 = self.embedding2(x2)\n",
    "        \n",
    "        # pack padded sequence\n",
    "#         embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, x1_lengths.numpy(), batch_first=True)\n",
    "#         embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, x2_lengths.numpy(), batch_first=True)\n",
    "        \n",
    "        # forward prop through each RNN \n",
    "        rnn1_out, self.hidden1 = self.rnn1(embed1, self.hidden1)\n",
    "        rnn2_out, self.hidden2 = self.rnn2(embed2, self.hidden2)\n",
    "        final_hidden1 = self.hidden1[self.num_layers - 1]\n",
    "        final_hidden2 = self.hidden2[self.num_layers - 1]\n",
    "        \n",
    "        # concat final hidden outputs and run them through FC layers \n",
    "        combined = torch.cat([final_hidden1, final_hidden2], dim=1)\n",
    "        output = F.relu(self.fc1(combined))\n",
    "        logits = self.fc2(output) \n",
    "\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to train and test model \n",
    "\n",
    "def test_model(loader, model): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    model.eval() \n",
    "    all_predictions = [] \n",
    "    all_labels = [] \n",
    "    \n",
    "    for sent1s, sent2s, sent1_lens, sent2_lens, labels in loader: \n",
    "        outputs = F.softmax(model(sent1s, sent2s, sent1_lens, sent2_lens), dim=1)\n",
    "        predictions = outputs.max(1, keepdim=True)[1]    \n",
    "        total += labels.size(0)\n",
    "        correct += predictions.eq(labels.view_as(predictions)).sum().item()\n",
    "        all_predictions += list(predictions.numpy().flatten()) \n",
    "        all_labels += list(labels.numpy().flatten())\n",
    "        \n",
    "    return (100 * correct / total), all_predictions, all_labels \n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, val_loader, num_epochs, learning_rate, print_intermediate=True):  \n",
    "    \"\"\"\n",
    "    Trains model on data from train_loader and evaluates on data from val_loader for num_epochs \n",
    "    Returns results as a dictionary comprising epoch, train accuracy, and validation accuracy \n",
    "    \"\"\"\n",
    "    # train and validate \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    results = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sent1s, sent2s, sent1_lens, sent2_lens, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sent1s, sent2s, sent1_lens, sent2_lens)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i % 100 == 0 or ((epoch==num_epochs-1) & (i==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + i / len(train_loader)\n",
    "                result['train_acc'], _, _ = test_model(train_loader, model)\n",
    "                result['val_acc'], _, _ = test_model(val_loader, model)       \n",
    "                results.append(result)\n",
    "\n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'.format(\n",
    "                        result['epoch'], result['train_acc'], result['val_acc']))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Accuracy: 33.20%, Validation Accuracy: 33.80%\n",
      "Epoch: 0.03, Train Accuracy: 38.85%, Validation Accuracy: 37.90%\n",
      "Epoch: 0.06, Train Accuracy: 40.65%, Validation Accuracy: 38.60%\n",
      "Epoch: 0.10, Train Accuracy: 40.93%, Validation Accuracy: 38.70%\n",
      "Epoch: 0.13, Train Accuracy: 41.14%, Validation Accuracy: 39.60%\n",
      "Epoch: 0.16, Train Accuracy: 38.12%, Validation Accuracy: 35.50%\n",
      "Epoch: 0.19, Train Accuracy: 41.16%, Validation Accuracy: 38.70%\n",
      "Epoch: 0.22, Train Accuracy: 42.14%, Validation Accuracy: 39.90%\n",
      "Epoch: 0.26, Train Accuracy: 41.46%, Validation Accuracy: 40.20%\n",
      "Epoch: 0.29, Train Accuracy: 41.09%, Validation Accuracy: 38.60%\n",
      "Epoch: 0.32, Train Accuracy: 39.96%, Validation Accuracy: 37.60%\n",
      "Epoch: 0.35, Train Accuracy: 41.02%, Validation Accuracy: 38.50%\n",
      "Epoch: 0.38, Train Accuracy: 40.88%, Validation Accuracy: 39.00%\n",
      "Epoch: 0.42, Train Accuracy: 41.79%, Validation Accuracy: 38.90%\n",
      "Epoch: 0.45, Train Accuracy: 37.14%, Validation Accuracy: 37.50%\n",
      "Epoch: 0.48, Train Accuracy: 41.32%, Validation Accuracy: 39.60%\n",
      "Epoch: 0.51, Train Accuracy: 41.99%, Validation Accuracy: 40.00%\n",
      "Epoch: 0.54, Train Accuracy: 41.84%, Validation Accuracy: 39.30%\n",
      "Epoch: 0.58, Train Accuracy: 41.82%, Validation Accuracy: 39.50%\n",
      "Epoch: 0.61, Train Accuracy: 41.79%, Validation Accuracy: 40.10%\n",
      "Epoch: 0.64, Train Accuracy: 41.32%, Validation Accuracy: 38.60%\n",
      "Epoch: 0.67, Train Accuracy: 41.25%, Validation Accuracy: 40.20%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-361-877b1954fd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnli_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnli_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-360-7488c8ed1cd0>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, print_intermediate)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-360-7488c8ed1cd0>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msent1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent1_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent1_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-350-07ee9e91cd37>\u001b[0m in \u001b[0;36mcollate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     48\u001b[0m         padded_vec1 = np.pad(array=np.array(datum[0]), \n\u001b[1;32m     49\u001b[0m                              \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SENTENCE_LENGTH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                              mode = 'constant', constant_values = 0)\n\u001b[0m\u001b[1;32m     51\u001b[0m         padded_vec2 = np.pad(array=np.array(datum[1]), \n\u001b[1;32m     52\u001b[0m                              \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SENTENCE_LENGTH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constant_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepend_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_append_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'edge'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_append_const\u001b[0;34m(arr, pad_amt, val, axis)\u001b[0m\n\u001b[1;32m    159\u001b[0m     padshape = tuple(x if i != axis else pad_amt\n\u001b[1;32m    160\u001b[0m                      for (i, x) in enumerate(arr.shape))\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_do_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# instantiate model and optimizer \n",
    "model = rnn_model(emb_size=300, hidden_size=10, num_layers=2, fc_out_size=10, num_classes=3, pretrained_weights=pretrained_weights)\n",
    "\n",
    "# train and evaluate \n",
    "results = train_and_eval(model, snli_train_loader, snli_val_loader, learning_rate=0.01, num_epochs=2, print_intermediate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
