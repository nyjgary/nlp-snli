{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librarties \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import nltk \n",
    "import spacy\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(134)\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab from pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab from fasttext embeddings \n",
    "\n",
    "def build_vocab(word2vec_source, max_vocab_size): \n",
    "    \"\"\" Takes pretrained word2vec source path, limits to max_vocab_size, and returns:  \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices \n",
    "        - word_emb: dictionary representing word embeddings \n",
    "    \"\"\"\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_source, limit=max_vocab_size)\n",
    "    id2token = word2vec_model.index2word\n",
    "    token2id = dict(zip(id2token, range(2, 2+len(id2token))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    word_emb = {token2id[w]: word2vec_model[w] for w in word2vec_model.vocab}\n",
    "\n",
    "    return id2token, token2id, word_emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token, token2id, word_emb = build_vocab('fasttext_word2vec/wiki-news-300d-1M.vec', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young girl in a pink shirt sitting on a dock...</td>\n",
       "      <td>A young girl watching the sunset over the water .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman is smiling while the man next to her i...</td>\n",
       "      <td>Two people are next to each other .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Across the river , you can see a large building .</td>\n",
       "      <td>The large building is full of apartments and t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a man in white shorts and a black shirt is par...</td>\n",
       "      <td>A man is riding a jetski on the ocean .</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four black dogs run together on bright green g...</td>\n",
       "      <td>Four dogs are preparing to be launched into sp...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A young girl in a pink shirt sitting on a dock...   \n",
       "1  A woman is smiling while the man next to her i...   \n",
       "2  Across the river , you can see a large building .   \n",
       "3  a man in white shorts and a black shirt is par...   \n",
       "4  Four black dogs run together on bright green g...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0  A young girl watching the sunset over the water .        neutral  \n",
       "1                Two people are next to each other .     entailment  \n",
       "2  The large building is full of apartments and t...        neutral  \n",
       "3            A man is riding a jetski on the ocean .  contradiction  \n",
       "4  Four dogs are preparing to be launched into sp...  contradiction  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_table('hw2_data/snli_train.tsv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228065"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sent1 = train['sentence1'].tolist()\n",
    "train_sent2 = train['sentence2'].tolist() \n",
    "# train_sent1_tokens = tokenize_dataset(train_sent1, lower_case_remove_punc) \n",
    "# train_sent2_tokens = tokenize_dataset(train_sent2, lower_case_remove_punc) \n",
    "train_sent1_tokens = [sent.split() for sent in train_sent1]\n",
    "train_sent2_tokens = [sent.split() for sent in train_sent2]\n",
    "all_train_tokens = [token for sent in train_sent1_tokens for token in sent] + [token for sent in train_sent2_tokens for token in sent] \n",
    "len(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.970e-02,  1.600e-02, -5.710e-02,  4.050e-02, -6.960e-02,\n",
       "       -1.237e-01,  3.010e-02,  2.480e-02, -3.030e-02,  1.740e-02,\n",
       "        6.300e-03,  1.840e-02,  2.170e-02, -2.570e-02,  3.500e-02,\n",
       "       -2.420e-02,  2.900e-03,  1.880e-02, -5.700e-02,  2.520e-02,\n",
       "       -2.100e-02, -8.000e-04,  3.600e-02, -7.290e-02, -6.650e-02,\n",
       "        9.890e-02,  6.760e-02,  8.520e-02, -8.900e-03,  3.130e-02,\n",
       "       -6.900e-03, -3.200e-03, -4.620e-02,  4.970e-02,  2.610e-02,\n",
       "        2.680e-02, -3.100e-02, -1.361e-01, -6.200e-03,  3.750e-02,\n",
       "       -3.200e-02, -1.060e-02,  5.340e-02, -1.870e-02,  6.380e-02,\n",
       "        9.400e-03,  4.700e-03, -5.300e-02,  9.300e-03, -8.700e-03,\n",
       "        4.000e-04,  4.930e-02, -6.296e-01,  2.220e-02,  1.900e-02,\n",
       "        2.680e-02, -4.260e-02,  5.700e-03, -1.683e-01,  2.440e-02,\n",
       "       -2.130e-02, -1.810e-02,  4.210e-02, -3.090e-02, -8.900e-03,\n",
       "        3.200e-03,  1.080e-02, -4.900e-03,  2.580e-02,  2.780e-02,\n",
       "       -1.630e-02,  2.000e-02,  1.640e-02, -9.540e-02, -3.200e-03,\n",
       "        4.300e-03,  1.040e-02, -8.800e-03,  7.000e-04,  3.500e-02,\n",
       "       -2.060e-02, -8.300e-03, -1.140e-02, -1.869e-01,  2.580e-02,\n",
       "        1.000e-03,  8.500e-03,  1.510e-02,  2.125e-01,  7.100e-03,\n",
       "        3.190e-02, -4.820e-02,  6.210e-02,  6.260e-02,  1.590e-02,\n",
       "       -1.300e-03,  8.700e-03,  6.860e-02, -3.400e-03,  2.380e-02,\n",
       "       -4.520e-02, -1.980e-02,  1.120e-02,  1.090e-02, -1.022e-01,\n",
       "       -2.720e-02,  2.337e-01, -4.650e-02,  1.592e-01, -4.070e-02,\n",
       "       -1.029e-01, -4.870e-02, -6.760e-02,  6.760e-02, -3.280e-02,\n",
       "        3.230e-02,  7.700e-03,  1.900e-02,  1.700e-03, -2.974e-01,\n",
       "        1.100e-03, -3.560e-02,  6.930e-02, -4.800e-02, -8.210e-02,\n",
       "       -6.440e-02, -2.840e-02, -1.910e-02, -2.330e-02,  3.530e-02,\n",
       "       -4.630e-02,  6.560e-02,  1.900e-03, -2.120e-02, -3.090e-02,\n",
       "       -3.534e-01, -3.090e-02,  7.600e-03, -4.190e-02,  4.570e-02,\n",
       "       -3.060e-02,  3.570e-02,  6.670e-02,  3.659e-01,  1.490e-02,\n",
       "       -4.430e-02,  6.800e-03, -3.780e-02,  1.460e-02,  2.150e-02,\n",
       "        1.081e-01,  1.240e-02, -4.370e-02, -4.300e-02,  2.580e-02,\n",
       "        2.130e-02, -3.090e-02, -1.800e-03, -6.700e-03,  1.720e-02,\n",
       "        8.900e-03, -1.710e-02,  2.750e-02, -5.180e-02, -1.840e-01,\n",
       "       -1.300e-02, -2.410e-02,  5.260e-02, -2.800e-02,  5.100e-03,\n",
       "        1.630e-02, -1.650e-02,  1.610e-02,  1.237e-01,  8.040e-02,\n",
       "       -7.890e-02,  3.860e-02, -3.892e-01,  1.570e-02, -2.460e-02,\n",
       "        4.770e-02, -4.500e-03, -2.140e-02,  1.730e-02, -1.910e-02,\n",
       "       -1.382e-01, -1.110e-02,  7.120e-02,  1.514e-01,  2.910e-02,\n",
       "        5.550e-02, -3.900e-03,  2.800e-03, -2.770e-02, -2.750e-02,\n",
       "       -1.770e-02, -3.380e-02, -3.720e-02,  2.071e-01,  4.600e-02,\n",
       "       -2.940e-02,  4.350e-02, -1.690e-02, -1.210e-02,  2.530e-02,\n",
       "        1.980e-02,  9.180e-02,  1.930e-02,  6.680e-02,  2.880e-02,\n",
       "        4.000e-03, -4.390e-02, -3.020e-02,  6.400e-03,  3.640e-02,\n",
       "        5.430e-02, -3.380e-02,  1.590e-02,  6.170e-02, -9.410e-02,\n",
       "       -8.600e-03, -9.200e-03,  3.000e-02, -2.410e-02, -3.500e-02,\n",
       "       -6.210e-02,  1.750e-02,  3.740e-02,  3.400e-03,  3.440e-02,\n",
       "        1.286e-01, -2.670e-02,  1.861e-01,  4.890e-02, -3.200e-03,\n",
       "        1.800e-02, -2.280e-02,  2.414e-01, -9.350e-02,  6.120e-02,\n",
       "       -2.090e-02,  1.360e-02,  3.920e-02, -1.350e-02, -2.530e-02,\n",
       "        3.350e-02,  9.500e-03,  4.190e-02,  7.600e-03,  4.522e-01,\n",
       "       -1.880e-02,  2.330e-02, -4.740e-02,  1.590e-02, -9.000e-03,\n",
       "        2.650e-02,  3.360e-02,  2.210e-02,  4.720e-02,  4.800e-03,\n",
       "        9.620e-02,  3.440e-02, -5.150e-02, -8.700e-03, -9.800e-02,\n",
       "       -2.880e-02,  3.770e-02,  2.020e-02, -2.979e-01, -3.870e-02,\n",
       "       -1.980e-02, -1.610e-02, -4.500e-03,  8.700e-03, -3.870e-02,\n",
       "        4.210e-02,  3.830e-02,  2.580e-02,  6.900e-03, -2.980e-02,\n",
       "       -1.980e-02, -1.520e-02,  3.300e-03,  7.500e-03,  3.580e-02,\n",
       "       -1.550e-02, -1.110e-02,  7.600e-02, -4.520e-02,  6.970e-02,\n",
       "        2.990e-02, -2.900e-03, -3.480e-02, -2.700e-02,  3.510e-02,\n",
       "        5.590e-02,  5.910e-02,  1.559e-01, -2.540e-02, -2.590e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_word2vec['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0730e-01  8.9000e-03  6.0000e-04  5.5000e-03 -6.4600e-02 -6.0000e-02\n",
      "  4.5000e-02 -1.3300e-02 -3.5700e-02  4.3000e-02 -3.5600e-02 -3.2000e-03\n",
      "  7.3000e-03 -1.0000e-04  2.5800e-02 -1.6600e-02  7.5000e-03  6.8600e-02\n",
      "  3.9200e-02  7.5300e-02  1.1500e-02 -8.7000e-03  4.2100e-02  2.6500e-02\n",
      " -6.0100e-02  2.4200e-01  1.9900e-02 -7.3900e-02 -3.1000e-03 -2.6300e-02\n",
      " -6.2000e-03  1.6800e-02 -3.5700e-02 -2.4900e-02  1.9000e-02 -1.8400e-02\n",
      " -5.3700e-02  1.4200e-01  6.0000e-02  2.2600e-02 -3.8000e-03 -6.7500e-02\n",
      " -3.6000e-03 -8.0000e-03  5.7000e-02  2.0800e-02  2.2300e-02 -2.5600e-02\n",
      " -1.5300e-02  2.2000e-03 -4.8200e-02  1.3100e-02 -6.0160e-01 -8.8000e-03\n",
      "  1.0600e-02  2.2900e-02  3.3600e-02  7.1000e-03  8.8700e-02  2.3700e-02\n",
      " -2.9000e-02 -4.0500e-02 -1.2500e-02  1.4700e-02  4.7500e-02  6.4700e-02\n",
      "  4.7400e-02  1.9900e-02  4.0800e-02  3.2200e-02  3.6000e-03  3.5000e-02\n",
      " -7.2300e-02 -3.0500e-02  1.8400e-02 -2.6000e-03  2.4000e-02 -1.6000e-02\n",
      " -3.0800e-02  4.3400e-02  1.4700e-02 -4.5700e-02 -2.6700e-02 -1.7030e-01\n",
      " -9.9000e-03  4.1700e-02  2.3500e-02 -2.6000e-02 -1.5190e-01 -1.1600e-02\n",
      " -3.0600e-02 -4.1300e-02  3.3000e-02  7.2300e-02  3.6500e-02 -1.0000e-04\n",
      "  4.2000e-03  3.4600e-02  2.7700e-02 -3.0500e-02  7.8400e-02 -4.0400e-02\n",
      "  1.8700e-02 -2.2500e-02 -2.0600e-02 -1.7900e-02 -2.4280e-01  6.6900e-02\n",
      "  5.2300e-02  5.2700e-02  1.4900e-02 -7.0800e-02 -9.8700e-02  2.6300e-02\n",
      " -6.1100e-02  3.0200e-02  2.1600e-02  3.1300e-02 -1.4000e-02 -2.4950e-01\n",
      " -3.4600e-02 -4.8000e-02  2.5000e-02  2.1300e-01 -3.3000e-02 -1.5530e-01\n",
      " -2.9200e-02 -3.4600e-02  1.0740e-01  1.0000e-03 -1.1700e-02 -5.7000e-03\n",
      " -1.2800e-01 -3.8000e-03  1.3000e-02 -1.1570e-01 -1.0800e-02  2.7500e-02\n",
      "  1.5800e-02 -1.6900e-02  7.0000e-03  2.4700e-02  5.1000e-02  1.0292e+00\n",
      " -2.8300e-02 -3.1000e-02 -2.6000e-03 -3.4300e-02  5.7800e-02  4.4400e-02\n",
      "  8.1200e-02 -2.1100e-02 -8.7200e-02  1.6900e-02  4.9900e-02  4.8500e-02\n",
      "  2.2700e-02 -3.2300e-02 -3.5000e-03  4.3500e-02 -2.7500e-02  1.5400e-02\n",
      "  1.3500e-02 -4.8400e-02 -6.9900e-02 -5.0200e-02  2.7450e-01 -3.0000e-04\n",
      " -3.7100e-02  5.1700e-02 -9.0800e-02  1.3000e-03  3.6000e-02  2.8000e-02\n",
      "  8.3900e-02  9.8000e-02 -4.9000e-02 -2.4230e-01 -1.4200e-02  2.4000e-03\n",
      " -2.0700e-02  1.2000e-03  8.8000e-03 -1.4300e-02 -1.9700e-02  5.1500e-02\n",
      " -8.5000e-03  2.5700e-02  2.1540e-01  3.0100e-02  2.1100e-02  5.3000e-02\n",
      " -5.0000e-04  1.7700e-02  1.6000e-03 -5.3000e-03 -1.6200e-02 -2.2300e-02\n",
      " -1.8620e-01  3.9800e-02  6.5800e-02 -9.6200e-02 -7.6000e-03 -7.5000e-03\n",
      " -3.4200e-02 -2.6500e-02  4.2000e-02  5.2200e-02 -2.6600e-02  2.0100e-02\n",
      " -1.3310e-01 -3.6700e-02  3.5100e-02  5.1800e-02 -8.7000e-03  5.9900e-02\n",
      " -1.0860e-01 -1.8800e-02  4.8100e-02  1.0500e-02 -6.0000e-03  1.5100e-02\n",
      " -3.1000e-03  7.7000e-03 -2.7600e-02 -3.7300e-02 -2.0300e-02  4.7200e-02\n",
      "  2.4600e-02  1.4400e-01  5.4200e-02 -2.2500e-02  2.4950e-01  1.6170e-01\n",
      "  3.8000e-03  1.1190e-01 -2.3000e-02 -7.8500e-02  2.5000e-02 -6.1600e-02\n",
      " -4.8500e-02  2.2500e-02  2.8100e-02  4.1000e-03  1.1200e-02  1.7200e-02\n",
      "  2.9100e-02 -2.8200e-02  2.6000e-03  4.0550e-01  3.9200e-02  8.8000e-03\n",
      "  2.2800e-02  2.9900e-02  1.1950e-01  5.4500e-02 -2.0000e-03  2.0000e-03\n",
      "  4.9000e-02  1.4500e-02 -8.6000e-03  9.8000e-03 -2.3600e-02  1.7100e-02\n",
      " -7.6500e-02 -4.0000e-02  1.2800e-02  1.1000e-03  4.2000e-03  2.4400e-02\n",
      "  7.5000e-03  2.0000e-02  2.0100e-02  1.9600e-02 -3.7700e-02 -4.3200e-02\n",
      " -7.3000e-03 -2.1000e-03  1.8300e-02  7.6000e-03  1.8050e-01 -5.5100e-02\n",
      "  7.5000e-03 -5.1600e-02  4.2000e-02 -6.8000e-03 -7.1100e-02 -1.4080e-01\n",
      "  5.0400e-02  2.7600e-02  4.7000e-02  3.2300e-02 -2.1900e-02  1.0000e-03\n",
      "  8.9000e-03  2.7600e-02  1.8600e-02  5.0000e-03  1.1730e-01 -4.0000e-02]\n"
     ]
    }
   ],
   "source": [
    "ft_words = []\n",
    "for word in ft_word2vec.vocab:\n",
    "    print(ft_word2vec[word])\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'the', '.', 'and', 'of']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
